{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #9 (Compilación)\n",
    "\n",
    "En esta clase estaremos implementando un **generador de lexer**. Nos apoyaremos en el intérprete de expresiones regulares que implementamos en la clase anterior.\n",
    "\n",
    "Por cuestión de comodidad, en esta clase usaremos una versión de autómata basada en referencias entre estados.\n",
    "La clase `State`, que usaremos para modelar los estados, se encuentra en `cmp.automata`. Pasemos a importala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.automata import State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El autómata resulta de la interconexión de estados. Cualquier estado puede usarse como raíz del autómata, pero en función del estado que se escoja será el lenguaje reconocido. Un estado se contruye a partir de un objeto, que se usará para representarlo. Este puede ser un entero, o string, pero también pueden ser otros estados a su vez. Además, es necesario especificar si el estado es final o no. Por defecto se asume que **no** es final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Estado 0 -------------\n",
      "Identificador: 0\n",
      "Es final: False\n",
      "Transiciones: {'a': [0], 'b': [1]}\n",
      "Epsilon transiciones: set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Estado 1 -------------\n",
      "Identificador: 1\n",
      "Es final: True\n",
      "Transiciones: {'a': [0], 'b': [1]}\n",
      "Epsilon transiciones: set()\n"
     ]
    }
   ],
   "source": [
    "a = State(0)\n",
    "b = State(1, True)\n",
    "a.add_transition('a', a)\n",
    "a.add_transition('b', b)\n",
    "b.add_transition('a', a)\n",
    "b.add_transition('b', b)\n",
    "\n",
    "display(a)\n",
    "\n",
    "print('----------- Estado 0 -------------')\n",
    "print('Identificador:', a.state)\n",
    "print('Es final:', a.final)\n",
    "print('Transiciones:', a.transitions)\n",
    "print('Epsilon transiciones:', a.epsilon_transitions)\n",
    "\n",
    "display(b)\n",
    "\n",
    "print('----------- Estado 1 -------------')\n",
    "print('Identificador:', b.state)\n",
    "print('Es final:', b.final)\n",
    "print('Transiciones:', b.transitions)\n",
    "print('Epsilon transiciones:', b.epsilon_transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las **transiciones** entre estados se añaden a través del método `<origin>.add_transition(symbol, end)`. En el caso de las **epsilon transiciones**, se usa `<origin>.add_epsilon_transition(end)`. Las transiciones entre estados son potencialmente no deterministas. Dependerá del autómata construido si se comporta de dicha forma o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = State(2)\n",
    "c.add_epsilon_transition(a)\n",
    "c.add_epsilon_transition(b)\n",
    "\n",
    "display(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se provee un mecanismo de conversión de autómata no determinista a determinista. Nótese que la transformación es solo sobre la estructura del autómata subyacente (se sigue usando la misma clase `State`). Los nodos del autómata original pasan ha formar parte del identificador de los nuevos estados: al consultar el campo `state`, se obtiene una tupla con los posibles estados en los que podría estar el autómata original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(c.to_deterministic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase `State` brinda el método `from_nfa` por compatibilidad con la API de autómatas que estuvimos usando hasta la clase anterior. Este método construye una instancia de `State` a partir de una instancia de `NFA` (o por tanto de `DFA`). Se puede incluir un segundo argumento al llamado del método `from_nfa` si se desea obtener, además del estado inicial del autómata resultante, el resto de los estados que fueron creados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (DFA):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.DFA at 0x257e4987a60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copia (State):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cmp.tools.automata import DFA\n",
    "\n",
    "automaton = DFA(states=3, finals=[2], transitions={\n",
    "    (0, 'a'): 0,\n",
    "    (0, 'b'): 1,\n",
    "    (1, 'a'): 2,\n",
    "    (1, 'b'): 1,\n",
    "    (2, 'a'): 0,\n",
    "    (2, 'b'): 1,\n",
    "})\n",
    "print('Original (DFA):')\n",
    "display(automaton)\n",
    "\n",
    "automaton = State.from_nfa(automaton)\n",
    "print('Copia (State):')\n",
    "display(automaton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, se provee un mecanismo de reconocimiento de cadenas (independientemente de si el autómata subyacente es determinista o no). Este es accesible a través del método `<state>.recognize(string)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert automaton.recognize('ba')\n",
    "assert automaton.recognize('aababbaba')\n",
    "\n",
    "assert not automaton.recognize('')\n",
    "assert not automaton.recognize('aabaa')\n",
    "assert not automaton.recognize('aababb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador de Lexer\n",
    "\n",
    "Como estudiamos en conferencia, el generador de lexer se basa en un conjunto de expresiones regulares. Cada una de ellas está asociada a un tipo de token. El lexer termina siendo un autómata finito determinista con ciertas peculiaridades:\n",
    "- Resulta de transformar el autómata unión entre todas las expresiones regulares del lenguaje, y convertirlo a determinista.\n",
    "- Cada estado final almacena los tipos de tokens que se reconocen al alcanzarlo. Se establece una prioridad entre ellos para poder desambiaguar.\n",
    "- Para tokenizar, la cadena de entrada viaja repetidas veces por el autómata.\n",
    "    - Cada vez, se comienza por el estado inicial, pero se continúa a partir de la última sección de la cadena que fue reconocida.\n",
    "    - Cuando el autómata se \"traba\" durante el reconocimiento de una cadena, se reporta la ocurrencia de un token. Su lexema está formado por la concatenación de los símbolos que fueron consumidos desde el inicio y hasta pasar por el último estado final antes de trabarse. Su tipo de token es el de mayor relevancia entre los anotados en el estado final.\n",
    "    - Al finalizar de consumir toda la cadena, se reporta el token de fin de cadena.\n",
    "\n",
    "### Expresiones regulares\n",
    "\n",
    "El engine de expresiones regulares que implementamos en la clase anterior está disponible en `cmp.tool.regex`. La clase `Regex` se instancia a partir de un string que representa la expresión regular. Cada instancia posee un campo `automaton` que permite acceder al `DFA` que reconoce el lenguaje denotado por dicha expresión regular. Las instancias de `Regex` son invocables, recibiendo como único parámentro un string, y devuelve si el string pertenece o no al lenguaje denotado por la expresión regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.regex import Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer\n",
    "\n",
    "Implementemos el generador de lexer. El lexer se construirá a partir de la tabla de expresiones regulares (una lista de tuplas con la forma: `(<token_type>, <regex_str>)`). Esta tabla se recibe como el parámetro `table` en el contructor de la clase `Lexer`. La prioridad/relevancia de cada tipo de token está marcada por el índice que ocupa en la tabla. Los tipos de tokens cuyas expresiones regulares estén registradas más cerca del inicio de la tabla tiene más prioridad.\n",
    "- **_build_regexs:** devuelve un lista con los autómatas (instancias de `State`) de cada expresión regular. Los estados finales de los respectivos autómatas deben marcarse (campo `tag`) con la prioridad y tipo de token según la expresión regular que lo originó.\n",
    "- **_build_automaton:** devuelve la versión determinista del autómata que reconoce los tokens del lenguaje.\n",
    "- **_walk:** Devuelve el último estado final visitado, y lexema consumido, durante el reconocimiento del string que se recibe como entrada.\n",
    "- **_tokenize:** Devuelve tuplas de la forma `(lex, token_type)` que resultan de tokenizar la entrada. Debe manejar el caso en que la entrada no puede ser tokenizada completamente (se detecta cuando en una iteración la cadena no avanzó)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.utils import Token\n",
    "\n",
    "class Lexer:\n",
    "    def __init__(self, table, eof):\n",
    "        self.eof = eof\n",
    "        self.regexs = self._build_regexs(table)\n",
    "        self.automaton = self._build_automaton()\n",
    "\n",
    "    def _build_regexs(self, table):\n",
    "        regexs = []\n",
    "        for n, (token_type, regex) in enumerate(table):\n",
    "            # Your code here!!!\n",
    "            # - Remember to tag the final states with the token_type and priority.\n",
    "            # - <State>.tag might be useful for that purpose ;-)\n",
    "            r = Regex(regex)\n",
    "            automata = r.automaton\n",
    "            start_state, states = State.from_nfa(automata, get_states=True)\n",
    "            for state in automata.finals:\n",
    "                states[state].tag = (token_type, n)\n",
    "            regexs.append(start_state)\n",
    "        return regexs\n",
    "\n",
    "    def _build_automaton(self):\n",
    "        start = State('start')\n",
    "        for state in self.regexs:\n",
    "            start.add_epsilon_transition(state)\n",
    "        return start.to_deterministic()\n",
    "\n",
    "    def _walk(self, string):\n",
    "        state = self.automaton\n",
    "        final = state if state.final else None\n",
    "        final_lex = lex = ''\n",
    "\n",
    "        for symbol in string:\n",
    "            lex += symbol\n",
    "            if symbol in state.transitions:\n",
    "                state = state.transitions[symbol][0]\n",
    "                if state.final:\n",
    "                    max_priority = len(self.regexs)\n",
    "                    for s in state.state:\n",
    "                        if s.final and s.tag[1] < max_priority:\n",
    "                            final = s.tag[0]\n",
    "                    final_lex = lex\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return final, final_lex\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        index = 0\n",
    "        \n",
    "        while index < len(text):\n",
    "            final, lex = self._walk(text[index:])\n",
    "            index += len(lex)\n",
    "            yield lex, final\n",
    "            \n",
    "        yield '$', self.eof\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return [Token(lex, ttype) for lex, ttype in self._tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyamos un lexer con algunas expresiones regulares solo para comprobar la validez de la implementación. Usaremos `str` para marcar los tipos de tokens, pero recuerde que realmente se usan los símbolos de la gramática ya que son los símbolos con los que trabaja el parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero digits: 1|2|3|4|5|6|7|8|9\n",
      "Letters: a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Tokenizando: \"5465 for 45foreach fore\"\n",
      "[num: 5465, space:  , for: for, space:  , num: 45, foreach: foreach, space:  , id: fore, eof: $]\n",
      "\n",
      ">>> Tokenizando: \"4forense forforeach for4foreach foreach 4for\"\n",
      "[num: 4, id: forense, space:  , id: forforeach, space:  , id: for4foreach, space:  , foreach: foreach, space:  , num: 4, for: for, eof: $]\n"
     ]
    }
   ],
   "source": [
    "nonzero_digits = '|'.join(str(n) for n in range(1,10))\n",
    "letters = '|'.join(chr(n) for n in range(ord('a'),ord('z')+1))\n",
    "\n",
    "print('Non-zero digits:', nonzero_digits)\n",
    "print('Letters:', letters)\n",
    "\n",
    "lexer = Lexer([\n",
    "    ('num', f'({nonzero_digits})(0|{nonzero_digits})*'),\n",
    "    ('for' , 'for'),\n",
    "    ('foreach' , 'foreach'),\n",
    "    ('space', '  *'),\n",
    "    ('id', f'({letters})({letters}|0|{nonzero_digits})*')\n",
    "], 'eof')\n",
    "\n",
    "text = '5465 for 45foreach fore'\n",
    "print(f'\\n>>> Tokenizando: \"{text}\"')\n",
    "tokens = lexer(text)\n",
    "print(tokens)\n",
    "assert [t.token_type for t in tokens] == ['num', 'space', 'for', 'space', 'num', 'foreach', 'space', 'id', 'eof']\n",
    "assert [t.lex for t in tokens] == ['5465', ' ', 'for', ' ', '45', 'foreach', ' ', 'fore', '$']\n",
    "\n",
    "text = '4forense forforeach for4foreach foreach 4for'\n",
    "print(f'\\n>>> Tokenizando: \"{text}\"')\n",
    "tokens = lexer(text)\n",
    "print(tokens)\n",
    "assert [t.token_type for t in tokens] == ['num', 'id', 'space', 'id', 'space', 'id', 'space', 'foreach', 'space', 'num', 'for', 'eof']\n",
    "assert [t.lex for t in tokens] == ['4', 'forense', ' ', 'forforeach', ' ', 'for4foreach', ' ', 'foreach', ' ', '4', 'for', '$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestas\n",
    "\n",
    "- Genere un lexer para el lenguaje de expresiones aritméticas que estuvimos trabajando a inicios del curso.\n",
    "- Implemente un algoritmo para eliminar los estados muertos de un autómata. Recordemos que esto es conveniente puesto que de esta forma el autómata del lexer detecta los tokens más de prisa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
