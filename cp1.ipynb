{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #1 (Compilación)\n",
    "\n",
    "A lo largo del curso estaremos implementando un compilador para el lenguaje de programación `HULK` (*Havana University Language for Kompilers*), paso a paso, introduciendo nuevas características del lenguaje o mejorando la implementación de otras características a medida que vamos descubriendo las técnicas fundamentales de la teoría de lenguajes y la compilación.\n",
    "\n",
    "El objetivo de esta clase es construir un evaluador de expresiones \"a mano\", usando los recursos que tenemos hasta el momento. Para ello vamos a comenzar con una versión de `HULK` muy sencilla, un lenguaje de expresiones aritméticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluador de expresiones\n",
    "\n",
    "Definiremos a continuación este lenguaje de manera informal:\n",
    "\n",
    "Un programa en `HULK` consta de una secuencia de expresiones. Cada expresión está compuesta por:\n",
    "\n",
    "- números (con coma flotante de 32 bits), \n",
    "- operadores `+ *` con el orden operacional, y\n",
    "- paréntesis `(` y `)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis lexicográfico\n",
    "\n",
    "Comenzaremos construyendo un prototipo bien simple, donde asumiremos que en la expresión hay espacios en blanco entre todos los elementos, de modo que el *lexer* se reduce a dividir por espacios. Luego iremos adicionando elementos más complejos.\n",
    "\n",
    "El siguiente método devuelve una lista de *tokens*, asumiendo que la expresión solo tiene números, operadores y paréntesis, separados por espacios en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for item in text.split():\n",
    "        if item.isnumeric():\n",
    "            tokens.append(float(item))\n",
    "        else:\n",
    "            tokens.append(item)\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "assert tokenize('5 + 6 * 9') == [5, '+', 6, '*', 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis sintáctico y evaluación\n",
    "\n",
    "Una vez que tenemos los *tokens*, solo nos queda evaluar la expresión. Usaremos para ello una idea simple, pero bien útil: evaluaremos recursivamente la expresión descendiendo por los distintos niveles de precedencia.\n",
    "\n",
    "Toda expresión del lenguaje puede ser vista como una suma de _términos_, donde cada uno de estos \"_términos_\" se descompone a su vez en operaciones de multiplicación entre _factores_. Incluso si no hay operadores `+` en toda la expresión queda claro que esta idea es válida puesto que estaríamos en presencia de una expresión formada por un solo _término_. Los _factores_ del lenguaje son todos unidades atómicas: por ahora solo números y expresiones complejas envueltas entre paréntesis. Nótese que el uso de paréntesis permite reiniciar el descenso por los niveles de precedencia (regresar a los niveles más altos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '*': lambda x,y: x * y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some util classes and methods\n",
    "\n",
    "class ParsingError(Exception):\n",
    "    \"\"\"\n",
    "    Base class for all parsing exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class BadEOFError(ParsingError):\n",
    "    \"\"\"\n",
    "    Unexpected EOF error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        ParsingError.__init__(self, \"Unexpected EOF\")\n",
    "        \n",
    "class UnexpectedToken(ParsingError):\n",
    "    \"\"\"\n",
    "    Unexpected token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Unexpected token: {token} at {i}')\n",
    "        \n",
    "class MissingCloseParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing ')' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \")\" token at {i}. Got \"{token}\" instead')\n",
    "        \n",
    "class MissingOpenParenthesisError(ParsingError):\n",
    "    \"\"\"\n",
    "    Missing '(' token error.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, token, i):\n",
    "        ParsingError.__init__(self, f'Expected \"(\" token at {i}. Got \"{token}\" instead')\n",
    "\n",
    "def get_token(tokens, i, error_type=BadEOFError):\n",
    "    \"\"\"\n",
    "    Returns tokens[i] if 'i' is in range. Otherwise, raises ParsingError exception.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return tokens[i]\n",
    "    except IndexError:\n",
    "        raise error_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokens):\n",
    "    \"\"\"\n",
    "    Evaluates an expression recursively.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        i, value = parse_expression(tokens, 0)\n",
    "        assert i == len(tokens)\n",
    "        return value\n",
    "    except ParsingError as error:\n",
    "        print(error)\n",
    "        return None\n",
    "\n",
    "def parse_expression(tokens, i):\n",
    "    i, term = parse_term(tokens, i)\n",
    "    return parse_group_of_terms(tokens, i, term)\n",
    "    \n",
    "def parse_group_of_terms(tokens, i, value):\n",
    "    if i < len(tokens):\n",
    "        if get_token(tokens, i) == '+':\n",
    "            i, term = parse_term(tokens, i + 1)\n",
    "            value = value + term\n",
    "            i, value = parse_group_of_terms(tokens, i, value)\n",
    "        elif get_token(tokens, i) == '-':\n",
    "            i, term = parse_term(tokens, i + 1)\n",
    "            value = value - term\n",
    "            i, value = parse_group_of_terms(tokens, i, value)\n",
    "    return i, value\n",
    "\n",
    "def parse_term(tokens, i):\n",
    "    i, factor = parse_factor(tokens, i)\n",
    "    return parse_group_of_factors(tokens, i, factor)\n",
    "\n",
    "def parse_group_of_factors(tokens, i, value):\n",
    "    if i < len(tokens):\n",
    "        if get_token(tokens, i) == '*':\n",
    "            i, factor = parse_factor(tokens, i + 1)\n",
    "            value = value * factor\n",
    "            i, value = parse_group_of_factors(tokens, i, value)\n",
    "        elif get_token(tokens, i) == '/':\n",
    "            i, factor = parse_factor(tokens, i + 1)\n",
    "            value = value / factor\n",
    "            i, value = parse_group_of_factors(tokens, i, value)\n",
    "    return i, value\n",
    "\n",
    "def parse_factor(tokens, i):\n",
    "    token = get_token(tokens, i)\n",
    "    if i < len(tokens):\n",
    "        if token == '(':\n",
    "            i, expr = parse_expression(tokens, i + 1)\n",
    "            if get_token(tokens, i) != ')':\n",
    "                raise MissingCloseParenthesisError(token, i)\n",
    "            return i + 1, expr \n",
    "        elif token == ')':\n",
    "            raise MissingOpenParenthesisError(token, i)\n",
    "    return i + 1, float(get_token(tokens, i))\n",
    "            \n",
    "assert evaluate(tokenize('5 + 6 * 9')) == 59.\n",
    "assert evaluate(tokenize('( 5 + 6 ) * 9')) == 99.\n",
    "assert evaluate(tokenize('( 5 + 6 ) + 1 * 9 + 2')) == 22.\n",
    "assert evaluate(tokenize('2 * 3 * 4 + 1')) == 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando constantes\n",
    "\n",
    "Agreguemos constantes numéricas al lenguaje `HULK` Para ello, simplemente añadiremos un diccionario con todas las constantes disponibles, que usaremos durante la tokenización. Nótese que solo es necesario modificar el _lexer_ para añadir este rasgo al lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = {\n",
    "    'pi': 3.14159265359,\n",
    "    'e': 2.71828182846,\n",
    "    'phi': 1.61803398875,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(expr):\n",
    "    \"\"\"\n",
    "    Returns the set of tokens. At this point, simply splits by \n",
    "    spaces and converts numbers to `float` instances.\n",
    "    Replaces constants.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    for token in expr.split():\n",
    "        if token.isnumeric():\n",
    "            tokens.append(float(token))\n",
    "        elif token in constants:\n",
    "            tokens.append(float(constants[token]))\n",
    "        else:\n",
    "            tokens.append(token)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "assert tokenize('2 * pi') == [2.0, '*', 3.14159265359]\n",
    "assert evaluate(tokenize('2 * pi')) == 6.28318530718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando operadores `-` y `/`\n",
    "\n",
    "- **Restricción:** No utilizar ciclos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These lambda expressions map from operators to actual executable code\n",
    "operations = {\n",
    "    '+': lambda x,y: x + y,\n",
    "    '-': lambda x,y: x - y,\n",
    "    '*': lambda x,y: x * y,\n",
    "    '/': lambda x,y: x / y,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_expression(tokens, i):\n",
    "#     # Insert your code here ...\n",
    "#     pass\n",
    "\n",
    "        \n",
    "# def parse_term(tokens, i):\n",
    "#     # Insert your code here ...\n",
    "#     pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert evaluate(tokenize('1 - 1 + 1')) == 1\n",
    "assert evaluate(tokenize('8 / 4 / 2')) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando funciones elementales\n",
    "\n",
    "Agreguemos funciones elementales `sin`, `cos`, `tan`, `log`, `sqrt`, etc. El llamado a funciones se hará en notación prefija, comenzando por el nombre de la función y seguido, entre paréntesis, por los argumentos, que estarán separados entre sí por _comas_.\n",
    "\n",
    "Para las funciones elementales haremos algo similar a las constantes, pero en vez de a la hora de tokenizar, las reemplazaremos a la hora de evaluar, pues necesitamos evaluar recursivamente los argumentos de la función. Empezaremos por garantizar que nuestro tokenizador que es capaz de reconocer expresiones con funciones elementales de más de un argumento, en caso de no ser así es necesario arreglarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert tokenize('log ( 64 , 1 + 3 )') == ['log', '(', 64.0, ',', 1.0, '+', 3.0, ')']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionaremos entonces un diccionario con todas las funciones elementales que permitiremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, modificaremos el método `evaluate` para que use las funciones elementales. Recordemos que los argumentos están separados por el token _coma_ (`,`) y que cada uno puede a su vez tener sub-expresiones que consistan también en llamados a funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_factor(tokens, i):\n",
    "    if i < len(tokens):\n",
    "        if get_token(tokens, i) == '(':\n",
    "            i, expr = parse_expression(tokens, i + 1)\n",
    "            if get_token(tokens, i) != ')':\n",
    "                raise MissingCloseParenthesisError(get_token(tokens, i), i)\n",
    "            return i + 1, expr\n",
    "\n",
    "        elif get_token(tokens, i) == ')':\n",
    "            raise MissingOpenParenthesisError(get_token(tokens, i), i)\n",
    "\n",
    "        elif get_token(tokens, i) == 'sin':\n",
    "            return one_parameter_function('sin', tokens, i)\n",
    "\n",
    "        elif get_token(tokens, i) == 'cos':\n",
    "            return one_parameter_function('cos', tokens, i)\n",
    "\n",
    "        elif get_token(tokens, i) == 'tan':\n",
    "            return one_parameter_function('tan', tokens, i)\n",
    "\n",
    "        elif get_token(tokens, i) == 'log':\n",
    "            return two_parameter_function('log', tokens, i)\n",
    "\n",
    "        elif get_token(tokens, i) == 'sqrt':\n",
    "            return one_parameter_function('sqrt', tokens, i)\n",
    "\n",
    "    return i + 1, get_token(tokens, i)\n",
    "\n",
    "def one_parameter_function(function, tokens, i):\n",
    "    if get_token(tokens, i + 1) != '(':\n",
    "        raise MissingOpenParenthesisError(tokens, i)\n",
    "    i, expr = parse_expression(tokens, i + 2)\n",
    "    expr = functions[function](expr)\n",
    "    if get_token(tokens, i) != ')':\n",
    "                raise MissingCloseParenthesisError(tokens, i)\n",
    "    return i + 1, expr\n",
    "\n",
    "def two_parameter_function(function, tokens, i):\n",
    "    if get_token(tokens, i + 1) != '(':\n",
    "        raise MissingOpenParenthesisError(tokens, i)\n",
    "    if get_token(tokens, i + 3) != ',':\n",
    "        raise UnexpectedToken(tokens, i)\n",
    "    i, expr1 = parse_expression(tokens, i + 2)\n",
    "    i, expr2 = parse_expression(tokens, i + 1)\n",
    "    expr = functions['log'](expr1, expr2)\n",
    "    if get_token(tokens, i) != ')':\n",
    "        raise MissingCloseParenthesisError(tokens, i)\n",
    "    return i + 1, expr\n",
    "    \n",
    "assert evaluate(tokenize('log ( 64 , 1 + 3 )')) == 3.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
