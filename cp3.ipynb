{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #3 (Compilación)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "El objetivo de esta clase es implementar un **generador automático de parsers LL(1)**. Tal generador recibiría como entrada la especificación de una gramática. Siguiendo esta idea, primeramente deberíamos ser capaces de representar de forma clara y cómoda tal \"especificación\" de la gramática.\n",
    "\n",
    "A continuación se describen un número de clases que nos serán útiles para construir, de forma concisa, una gramática como un objeto de **Python**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Símbolos\n",
    "\n",
    "Modelaremos los **símbolos** del lenguaje con la clase `Symbol`. Esta clase funcionará como base para la definición de terminales y no terminales. Entre las funcionalidades básicas de los símbolos tenemos que:\n",
    "- Pueden ser agrupados con el operador `+` para formar oraciones.\n",
    "- Podemos conocer si representa la cadena especial **epsilon** a través de la propiedad `IsEpsilon` que poseen todas las instancias.\n",
    "- Podemos acceder a la gramática en la que se definió a través del campo `Grammar` de cada instancia.\n",
    "- Podemos consultar la notación del símbolo a través del campo `Name` de cada instancia.\n",
    "\n",
    "Los símbolos no deben ser instanciados directamente (ni sus descendiente) con la aplicación de su constructor. En su lugar, utilizaremos una sintaxis descrita más adelante para definirlos junto a la especificación de la gramática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Terminales\n",
    "\n",
    "Los símbolos **no terminales** los modelaremos con la clase `NonTerminal`. Dicha clase extiende la clase `Symbol` para:\n",
    "- Añadir noción de las producción que tiene al no terminal como cabecera. Estas pueden ser conocidas a través del campo `productions` de cada instancia.\n",
    "- Permitir añadir producciones para ese no terminal a través del operador `%=`.\n",
    "- Incluir propiedades `IsNonTerminal` - `IsTerminal` que devolveran `True` - `False` respectivamente.\n",
    "\n",
    "Los no terminales no deben ser instanciados directamente con la aplicación de su constructor. En su lugar, se presentan las siguientes facilidades para definir no terminales a partir de una instancia `G` de `Grammar`:\n",
    "- Para definir un único no terminal:\n",
    "    ```python\n",
    "    non_terminal_var = G.NonTerminal('<non-terminal-name>')\n",
    "    # non_terminal_var    <--- variable en la que guardaremos la referencia al no terminal.\n",
    "    # <non-terminal-name> <--- nombre concreto del no terminal.\n",
    "    ```\n",
    "- Para definir el símbolo distingido:\n",
    "    ```python\n",
    "    start_var = G.NonTerminal('<start-name>', True)\n",
    "    # start_var    <--- variable en la que guardaremos la referencia símbolo distinguido.\n",
    "    # <start-name> <--- nombre concreto del símbolo distinguido.\n",
    "    ```\n",
    "- Para definir múltiples no terminales:\n",
    "    ```python\n",
    "    var1, var2, ..., varN = G.NonTerminals('<name1> <name2> ... <nameN>')\n",
    "    # var1, var2, ..., varN    <--- variables en las que guardaremos las referencias a los no terminales.\n",
    "    # <name1> <name2> ... <nameN> <--- nombres concretos del no terminales (separados por espacios).\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import NonTerminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminales\n",
    "\n",
    "Los símbolos **terminales** los modelaremos con la clase `Terminal`. Dicha clase extiende la clase `Symbol` para:\n",
    "- Incluir propiedades `IsNonTerminal` - `IsTerminal` que devolveran `True` - `False` respectivamente.\n",
    "\n",
    "Los terminales no deben ser instanciados directamente con la aplicación de su constructor. En su lugar, se presentan las siguientes facilidades para definir no terminales a partir de una instancia `G` de `Grammar`:\n",
    "- Para definir un único terminal:\n",
    "    ```python\n",
    "    terminal_var = G.Terminal('<terminal-name>')\n",
    "    # terminal_var    <--- variable en la que guardaremos la referencia al terminal.\n",
    "    # <terminal-name> <--- nombre concreto del terminal.\n",
    "    ```\n",
    "- Para definir múltiples terminales:\n",
    "    ```python\n",
    "    var1, var2, ..., varN = G.Terminals('<name1> <name2> ... <nameN>')\n",
    "    # var1, var2, ..., varN    <--- variables en las que guardaremos las referencias a los terminales.\n",
    "    # <name1> <name2> ... <nameN> <--- nombres concretos del terminales (separados por espacios).\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fin de Cadena (EOF, *$*)\n",
    "Modelaremos el símbolo de fin de cadena con la clase `EOF`. Dicha clase extiende la clases `Terminal` para heradar su comportamiento.\n",
    "\n",
    "La clase `EOF` no deberá ser instanciada directamente con la aplicación de su constructor. En su lugar, una instancia concreta para determinada gramática `G` de `Grammar` se construirá automáticamente y será accesible a través de `G.EOF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oraciones y Formas Oracionales\n",
    "\n",
    "Modelaremos los **oraciones** y **formas oracionales** del lenguaje con la clase `Sentence`. Esta clase funcionará como una colección de terminales y no terminales. Entre las funcionalidades básicas que provee tenemos que nos :\n",
    "- Permite acceder a los símbolos que componen la oración a través del campo `_symbols` de cada instancia.\n",
    "- Permite conocer si la oración es completamente vacía a través de la propiedad `IsEpsilon`.\n",
    "- Permite obtener la concatenación con un símbolo u otra oración aplicando el operador `+`.\n",
    "- Permite conocer la longitud de la oración (cantidad de símbolos que la componen) utilizando la función *build-in* de python `len(...)`.\n",
    "\n",
    "Las oraciones pueden ser agrupadas usando el operador `|`. Esto nos será conveniente para definir las producciones las producciones que tengan la misma cabeza (no terminal en la parte izquierda) en una única sentencia. El grupo de oraciones se maneja con la clase `SentenceList`.\n",
    "\n",
    "No se deben crear instancias de `Sentence` y `SentenceList` directamente con la aplicación de los respectivos constructores. En su lugar, usaremos el operador `+` entre símbolos para formar las oraciones, y el operador `|` entre oraciones para agruparlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Sentence, SentenceList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon\n",
    "\n",
    "Modelaremos tanto la **cadena vacía** como el símbolo que la representa: **epsilon** ($\\epsilon$), en la misma clase: `Epsilon`. Dicha clase extiende las clases `Terminal` y `Sentence` por lo que ser comporta como ambas. Sobreescribe la implementación del método `IsEpsilon` para indicar que en efecto toda instancia de la clase reprensenta **epsilon**.\n",
    "\n",
    "La clase `Epsilon` no deberá ser instanciada directamente con la aplicación de su constructor. En su lugar, una instancia concreta para determinada gramática `G` de `Grammar` se construirá automáticamente y será accesible a través de `G.Epsilon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producciones\n",
    "\n",
    "Modelaremos las **producciones** con la clase `Production`. Las funcionalidades básicas con que contamos son:\n",
    "- Poder acceder la cabecera (parte izquierda) y cuerpo (parte derecha) de cada producción a través de los campos `Left` y `Right` respectivamente.\n",
    "- Consultar si la producción es de la forma $X \\rightarrow \\epsilon$ a través de la propiedad `IsEpsilon`.\n",
    "- Desempaquetar la producción en cabecera y cuerpo usando asignaciones: `left, right = production`.\n",
    "\n",
    "Las producciones no deben ser instanciadas directamente con la aplicación de su constructor. En su lugar, se presentan las siguientes facilidades para formar producciones a partir de una instancia `G` de `Grammar` y un grupo de terminales y no terminales:\n",
    "- Para definir una producción de la forma $E \\rightarrow E + T$:\n",
    "    ```python\n",
    "    E %= E + plus + T\n",
    "    ```\n",
    "- Para definir múltiples producciones de la misma cabecera en una única sentencia ($E \\rightarrow$ $E + T$ | $E - T$ | $T$):\n",
    "    ```python\n",
    "    E %= E + plus + T | E + minus + T | T\n",
    "    ```\n",
    "- Para usar *epsilon* en una producción (ejemplo $S \\rightarrow$ $aS$ | $\\epsilon$) haríamos:\n",
    "    ```python\n",
    "    S %= S + a | G.Epsilon\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramática\n",
    "\n",
    "Modelaremos las **gramáticas** con la clase `Grammar`. Las funcionalidades básicas con que contamos son:\n",
    "- Definir los símbolos _terminales_ y _no terminales_ de la gramática en cuestión a través de los métodos `Terminal` y `Terminals` para los primeros, y `NonTerminal` y `NonTerminals` para los segundos.\n",
    "- Definir las producciones de la gramática a partir de la aplicación del operador `%=` entre no terminales y oraciones (estas a su vez formadas por la concatenación de símbolos).\n",
    "- Acceder a **todas** las _producciones_ a través del campo `Productions` de cada instancia.\n",
    "- Acceder a **todos** los _terminales_ y _no terminales_ a través de los campos `terminals` y `nonTerminals` respectivamente.\n",
    "- Acceder al _símbolo distinguido_, _epsilon_ y _fin de cadena_ a través de los campos `startSymbol`, `Epsilon` y `EOF` respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.pycompiler import Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplo: Gramática de HULK\n",
    "\n",
    "A continuación de muestra cómo especificar la gramática LL(1) para `HULK` vista en conferencias:\n",
    "\n",
    "$ E \\rightarrow$ $TX$  \n",
    "$ X \\rightarrow$ $+TX$ | $-TX$ | $\\epsilon$  \n",
    "$ T \\rightarrow$ $FY$  \n",
    "$ Y \\rightarrow$ $*FY$ | $/FY$ | $\\epsilon$  \n",
    "$ F \\rightarrow$ $( E )$ | num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Terminals:\n",
      "\tE, T, F, X, Y\n",
      "Terminals:\n",
      "\t+, -, *, /, (, ), num\n",
      "Productions:\n",
      "\t[E -> T X, X -> + T X, X -> - T X, X -> e, T -> F Y, Y -> * F Y, Y -> / F Y, Y -> e, F -> num, F -> ( E )]\n"
     ]
    }
   ],
   "source": [
    "from cmp.utils import pprint, inspect\n",
    "\n",
    "G = Grammar()\n",
    "E = G.NonTerminal('E', True)\n",
    "T,F,X,Y = G.NonTerminals('T F X Y')\n",
    "plus, minus, star, div, opar, cpar, num = G.Terminals('+ - * / ( ) num')\n",
    "\n",
    "E %= T + X\n",
    "X %= plus + T + X | minus + T + X | G.Epsilon\n",
    "T %= F + Y\n",
    "Y %= star + F + Y | div + F + Y | G.Epsilon\n",
    "F %= num | opar + E + cpar\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generador de parsers LL(1)\n",
    "\n",
    "Pasemos a implementar un generador de parsers LL(1). Para ello será necesario implementar un algoritmo que compute el conjunto _First_ de los símbolos terminales, no terminales y producciones. Además, implementaremos un algoritmo para computar el _Follow_ de todos los no terminales de la gramática.\n",
    "\n",
    "Una vez podamos calcular dichos conjuntos, construiremos una tabla con $|V_N|$ filas y $|V_t| + 1$ columnas, que representará la unidad de control del parser. La celda $(i,j)$ de la tabla contiene la producción a aplicar si al \"tratar\" de parsear el _i-ésimo_ no terminal, el cabezal queda apuntando al símbolo asociado a la columna _j_ (es un terminal o _$_). Aplicar una producción se reduce a consumir terminales y tratar de parsear no terminales, según van apareciendo en la parte derecha de la producción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de símbolos\n",
    "\n",
    "Resulta conveniente manejar la pertenencia o no de *epsilon* a un conjunto como un caso extremo. Para ello usaremos la clase `ContainerSet` implementada a continuación.\n",
    "- La clase funciona como un conjunto de símbolos.\n",
    "- Permite consulta la pertenencia de _epsilon_ al conjunto.\n",
    "- Las operaciones que modifican el conjunto devuelven si hubo _cambio_ o _no_.\n",
    "- El conjunto puede ser actualizado con la adición de elementos individuales, `add(...)`, o a partir de otro conjunto,`update(...)` y `hard_update(...)`.\n",
    "- La actualización _sin epsilon (1)_, _con epsilon (2)_ y de _solo epsilon (3)_, ocurre a través de `update(...)`, `hard_update(...)` y `epsilon_update(...)` respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContainerSet:\n",
    "    def __init__(self, *values, contains_epsilon=False):\n",
    "        self.set = set(values)\n",
    "        self.contains_epsilon = contains_epsilon\n",
    "        \n",
    "    def add(self, value):\n",
    "        n = len(self.set)\n",
    "        self.set.add(value)\n",
    "        return n != len(self.set)\n",
    "        \n",
    "        \n",
    "    def set_epsilon(self, value=True):\n",
    "        last = self.contains_epsilon\n",
    "        self.contains_epsilon = value\n",
    "        return last != self.contains_epsilon\n",
    "        \n",
    "    def update(self, other):\n",
    "        n = len(self.set)\n",
    "        self.set.update(other.set)\n",
    "        return n != len(self.set)\n",
    "    \n",
    "    def epsilon_update(self, other):\n",
    "        return self.set_epsilon(self.contains_epsilon | other.contains_epsilon)\n",
    "    \n",
    "    def hard_update(self, other):\n",
    "        return self.update(other) | self.epsilon_update(other)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.set) + int(self.contains_epsilon)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '%s-%s' % (str(self.set), self.contains_epsilon)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.set)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, ContainerSet) and self.set == other.set and self.contains_epsilon == other.contains_epsilon\n",
    "\n",
    "# Simplemente para usar la definión del modulo cmp\n",
    "from cmp.utils import ContainerSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First de una forma oracional\n",
    "\n",
    "Recordemos que el conjunto *First* de una forma oracional se define como:\n",
    "- $First(w) = \\{ x \\in V_t $ | $ w \\Rightarrow^* x \\alpha, \\alpha \\in (V_t \\cup V_n)^* \\}$\n",
    "    - $\\cup$ $\\{ \\epsilon  \\}$, si $w \\Rightarrow^* \\epsilon$\n",
    "    - $\\cup$ $\\{  \\}$, en otro caso.\n",
    "    \n",
    "Como vimos en conferencia, es posible computar el conjunto _First_ para los _terminales_, _no terminales_ y _producciones_ de la gramática usando un método de punto fijo. Los _firsts_ se inicializan vacíos y de forma incremental se van actualizando con la aplicación de las siguientes reglas:\n",
    "- Si `X` $\\rightarrow$ `W1 | W2 | ... | Wn` entonces por definición, First(X) = $\\cup_i$ First($W_i$).\n",
    "- Si `X` $\\rightarrow \\epsilon$ entonces $\\epsilon \\in$ `First(X)`.\n",
    "- Si `W = xZ` donde `x` es un terminal, entonces trivialmente `First(W) = { x }`.\n",
    "- Si `W = YZ` donde `Y` es un no-terminal y `Z` una forma oracional, entonces `First(Y)` $\\subseteq$ `First(W)`.\n",
    "- Si `W = YZ` y $\\epsilon \\in$ `First(Y)` entonces `First(Z)` $\\subseteq$ `First(W)`.\n",
    "\n",
    "Una vez se termine una iteración sin que ocurran cambios se puede dar por terminado el calculo de los _firsts_.\n",
    "\n",
    "Implementemos el algoritmo para calcular el _first_ de los símbolos y producciones de la gramática en dos fases:\n",
    "- Con el método `compute_local_first` calcularemos `First(alpha)`, donde `alpha` es una forma oracional, según la versión \"actual\" del conjunto _firsts_ ya computada.\n",
    "- Con el método `compute_firsts` calcularemos todos los conjuntos _firsts_. Para ello, realizaremos actualizaciones a los conjuntos iniciales según los resultados de aplicar `compute_local_first` en cada producción.\n",
    "\n",
    "#### Primera fase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes First(alpha), given First(Vt) and First(Vn) \n",
    "# alpha in (Vt U Vn)*\n",
    "def compute_local_first(firsts, alpha):\n",
    "    first_alpha = ContainerSet()\n",
    "    \n",
    "    try:\n",
    "        alpha_is_epsilon = alpha.IsEpsilon\n",
    "    except:\n",
    "        alpha_is_epsilon = False\n",
    "    \n",
    "    ###################################################\n",
    "    # alpha == epsilon ? First(alpha) = { epsilon }\n",
    "    ###################################################\n",
    "    #                   <CODE_HERE>                   #\n",
    "    ###################################################\n",
    "    \n",
    "    ###################################################\n",
    "    # alpha = X1 ... XN\n",
    "    # First(Xi) subconjunto First(alpha)\n",
    "    # epsilon pertenece a First(X1)...First(Xi) ? First(Xi+1) subconjunto de First(X) y First(alpha)\n",
    "    # epsilon pertenece a First(X1)...First(XN) ? epsilon pertence a First(X) y al First(alpha)\n",
    "    ###################################################\n",
    "    #                   <CODE_HERE>                   #\n",
    "    ###################################################\n",
    "    \n",
    "    # First(alpha)\n",
    "    return first_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segunda fase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes First(Vt) U First(Vn) U First(alpha)\n",
    "# P: X -> alpha\n",
    "def compute_firsts(G):\n",
    "    firsts = {}\n",
    "    change = True\n",
    "    \n",
    "    # init First(Vt)\n",
    "    for terminal in G.terminals:\n",
    "        firsts[terminal] = ContainerSet(terminal)\n",
    "        \n",
    "    # init First(Vn)\n",
    "    for nonterminal in G.nonTerminals:\n",
    "        firsts[nonterminal] = ContainerSet()\n",
    "    \n",
    "    while change:\n",
    "        change = False\n",
    "        \n",
    "        # P: X -> alpha\n",
    "        for production in G.Productions:\n",
    "            X = production.Left\n",
    "            alpha = production.Right\n",
    "            \n",
    "            # get current First(X)\n",
    "            first_X = firsts[X]\n",
    "                \n",
    "            # init First(alpha)\n",
    "            try:\n",
    "                first_alpha = firsts[alpha]\n",
    "            except KeyError:\n",
    "                first_alpha = firsts[alpha] = ContainerSet()\n",
    "            \n",
    "            # CurrentFirst(alpha)???\n",
    "            local_first = compute_local_first(firsts, alpha)\n",
    "            \n",
    "            # update First(X) and First(alpha) from CurrentFirst(alpha)\n",
    "            change |= first_alpha.hard_update(local_first)\n",
    "            change |= first_X.hard_update(local_first)\n",
    "                    \n",
    "    # First(Vt) + First(Vt) + First(RightSides)\n",
    "    return firsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobemos que el algoritmo está bien implementado. Los conjuntos _First_ resultantes deberían ser:\n",
    "\n",
    "**Terminales**\n",
    "```\n",
    "+   :  { + }\n",
    "-   :  { - }\n",
    "*   :  { * }\n",
    "/   :  { / }\n",
    "(   :  { ( }\n",
    ")   :  { ) }\n",
    "num :  { num }\n",
    "```\n",
    "\n",
    "**No Terminales**\n",
    "```\n",
    "E  :  { num, ( }\n",
    "T  :  { num, ( }\n",
    "F  :  { num, ( }\n",
    "X  :  { +, -, epsilon }\n",
    "Y  :  { /, *, epsilon }\n",
    "```\n",
    "\n",
    "**Producciones**\n",
    "```\n",
    "T X     :  { num, ( }\n",
    "+ T X   :  { + }\n",
    "- T X   :  { - }\n",
    "epsilon :  { epsilon }\n",
    "F Y     :  { num, ( }\n",
    "* F Y   :  { * }\n",
    "/ F Y   :  { / }\n",
    "num     :  { num }\n",
    "( E )   :  { ( }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.languages import BasicHulk\n",
    "hulk = BasicHulk(G)\n",
    "\n",
    "firsts = compute_firsts(G)\n",
    "assert firsts == hulk.firsts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Follows\n",
    "\n",
    "Recordemos que el conjunto _Follow_ de un _no terminal_ se define como:\n",
    "- $Follow(X) = \\{ a \\in V_t \\cup \\{ \\$ \\} $ | $ S\\$ \\Rightarrow^* \\alpha X a \\beta , \\alpha \\in (V_t \\cup V_n)^* \\},  \\beta \\in (V_t \\cup V_n \\cup \\{ \\$ \\})^* \\}$\n",
    "\n",
    "Para calcular los _follows_ de los no terminales procederemos de forma similar a como hicimos con los _firsts_. Aplicaremos un método de punto fijo según las reglas:\n",
    "- `$` pertenece al `Follow(S)`.\n",
    "- Por definición `epsilon` nunca pertenece al `Follow(X)` para todo `X`.\n",
    "- Si `X` $\\rightarrow$ `WAZ` siendo `W` y `Z` formas oracionales, y `A` un no-terminal cualquiera, entonces `First(Z) - {` $\\epsilon$ `}` $\\subseteq$ `Follow(A)`.\n",
    "- Si `X` $\\rightarrow$ `WAZ` y $\\epsilon \\in$ `First(Z)`, entonces `Follow(X)` $\\subseteq$ `Follow(A)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def compute_follows(G, firsts):\n",
    "    follows = { }\n",
    "    change = True\n",
    "    \n",
    "    local_firsts = {}\n",
    "    \n",
    "    # init Follow(Vn)\n",
    "    for nonterminal in G.nonTerminals:\n",
    "        follows[nonterminal] = ContainerSet()\n",
    "    follows[G.startSymbol] = ContainerSet(G.EOF)\n",
    "    \n",
    "    while change:\n",
    "        change = False\n",
    "        \n",
    "        # P: X -> alpha\n",
    "        for production in G.Productions:\n",
    "            X = production.Left\n",
    "            alpha = production.Right\n",
    "            \n",
    "            follow_X = follows[X]\n",
    "            \n",
    "            ###################################################\n",
    "            # X -> zeta Y beta\n",
    "            # First(beta) - { epsilon } subset of Follow(Y)\n",
    "            # beta ->* epsilon or X -> zeta Y ? Follow(X) subset of Follow(Y)\n",
    "            ###################################################\n",
    "            #                   <CODE_HERE>                   #\n",
    "            ###################################################\n",
    "\n",
    "    # Follow(Vn)\n",
    "    return follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobemos que el algoritmo está bien implementado. Los conjuntos _Follow_ resultantes deberían ser:\n",
    "\n",
    "```\n",
    "E :  { ), $ }\n",
    "T :  { +, ), -, $ }\n",
    "F :  { /, +, ), *, -, $ }\n",
    "X :  { ), $ }\n",
    "Y :  { +, ), -, $ }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follows = compute_follows(G, firsts)\n",
    "assert follows == hulk.follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabla LL(1)\n",
    "\n",
    "Una vez tenemos todos los conjuntos `First` y `Follow` calculados, construiremos una tabla `T`, donde asociaremos a cada par no-terminal `X` / token `t` una producción (a lo sumo). Dicha producción es la única que tiene sentido aplicar si se debe expandir el no-terminal `X` y el token actual es `t`.\n",
    "\n",
    "Las reglas generales para generar esta tabla son las siguientes:\n",
    "\n",
    "1. Si `X` $\\to$ `W` y `t` $\\in V_t$ pertenece al `First(W)` entonces `T[X,t] = X` $\\to$ `W`.\n",
    "2. Si `X` $\\to$ `W` con $\\epsilon \\in$ `First(W)` y `t` pertenece al `Follow(X)` entonces `T[X,t] = X` $\\to$ `W`.\n",
    "\n",
    "Si al aplicar estas reglas, en cada posición `T[X,t]` obtenemos a lo sumo una producción, entonces decimos que una gramática es LL(1). En caso contrario, tenemos al menos un conflicto, pues hay más de una producción que tiene sentido utilizar en algún caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parsing_table(G, firsts, follows):\n",
    "    # init parsing table\n",
    "    M = {}\n",
    "    \n",
    "    # P: X -> alpha\n",
    "    for production in G.Productions:\n",
    "        X = production.Left\n",
    "        alpha = production.Right\n",
    "        \n",
    "        ###################################################\n",
    "        # working with symbols on First(alpha) ...\n",
    "        ###################################################\n",
    "        #                   <CODE_HERE>                   #\n",
    "        ###################################################    \n",
    "        \n",
    "        \n",
    "        ###################################################\n",
    "        # working with epsilon...\n",
    "        ###################################################\n",
    "        #                   <CODE_HERE>                   #\n",
    "        ###################################################\n",
    "    \n",
    "    # parsing table is ready!!!\n",
    "    return M            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobemos que el algoritmo está bien implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = build_parsing_table(G, firsts, follows)\n",
    "assert M == hulk.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Descendente No Recursivo\n",
    "\n",
    "Una vez obtenida la tabla LL(1) podemos escribir un algoritmo de parsing descendente no recursivo. La idea general consiste en emplear una pila de símbolos, donde iremos construyendo la forma oracional que eventualmente derivará en la cadena a reconocer. Si leemos la pila desde el tope hasta el fondo, en todo momento tendremos una forma oracional que debe generar la parte de la cadena no reconocida.\n",
    "\n",
    "El símbolo en el tope de la pila representa el terminal o no-terminal a analizar. En caso de ser un terminal, debe coincidir con el token analizado. En caso de ser un no-terminal, se consulta la tabla LL(1) y se ejecuta la producción correspondiente, insertando en la pila (en orden inverso) la forma oracional en que deriva el no-terminal extraído."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metodo_predictivo_no_recursivo(G, M=None, firsts=None, follows=None):\n",
    "    \n",
    "    # checking table...\n",
    "    if M is None:\n",
    "        if firsts is None:\n",
    "            firsts = compute_firsts(G)\n",
    "        if follows is None:\n",
    "            follows = compute_follows(G, firsts)\n",
    "        M = build_parsing_table(G, firsts, follows)\n",
    "    \n",
    "    \n",
    "    # parser construction...\n",
    "    def parser(w):\n",
    "        \n",
    "        ###################################################\n",
    "        # w ends with $ (G.EOF)\n",
    "        ###################################################\n",
    "        # init:\n",
    "        ### stack =  ????\n",
    "        ### cursor = ????\n",
    "        ### output = ????\n",
    "        ###################################################\n",
    "        \n",
    "        # parsing w...\n",
    "        while True:\n",
    "            top = stack.pop()\n",
    "            a = w[cursor]\n",
    "            \n",
    "            ###################################################\n",
    "            #                   <CODE_HERE>                   #\n",
    "            ###################################################\n",
    "\n",
    "        # left parse is ready!!!\n",
    "        return output\n",
    "    \n",
    "    # parser is ready!!!\n",
    "    return parser\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobemos que el algoritmo está bien implementado. El parse izquierdo para la cadena `\"n * n * n + n * n + n + n $\"` es:\n",
    "```\n",
    "E -> T X\n",
    "T -> F Y\n",
    "F -> num\n",
    "Y -> * F Y\n",
    "F -> num\n",
    "Y -> * F Y\n",
    "F -> num\n",
    "Y -> e\n",
    "X -> + T X\n",
    "T -> F Y\n",
    "F -> num\n",
    "Y -> * F Y\n",
    "F -> num\n",
    "Y -> e\n",
    "X -> + T X\n",
    "T -> F Y\n",
    "F -> num\n",
    "Y -> e\n",
    "X -> + T X\n",
    "T -> F Y\n",
    "F -> num\n",
    "Y -> e\n",
    "X -> e\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = metodo_predictivo_no_recursivo(G, M)\n",
    "left_parse = parser([num, star, num, star, num, plus, num, star, num, plus, num, plus, num, G.EOF])\n",
    "\n",
    "assert left_parse == [ \n",
    "   Production(E, Sentence(T, X)),\n",
    "   Production(T, Sentence(F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, Sentence(star, F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, Sentence(star, F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, G.Epsilon),\n",
    "   Production(X, Sentence(plus, T, X)),\n",
    "   Production(T, Sentence(F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, Sentence(star, F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, G.Epsilon),\n",
    "   Production(X, Sentence(plus, T, X)),\n",
    "   Production(T, Sentence(F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, G.Epsilon),\n",
    "   Production(X, Sentence(plus, T, X)),\n",
    "   Production(T, Sentence(F, Y)),\n",
    "   Production(F, Sentence(num)),\n",
    "   Production(Y, G.Epsilon),\n",
    "   Production(X, G.Epsilon),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos\n",
    "\n",
    "Probemos el generador de parser implementado con otras gramáticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramática 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Grammar()\n",
    "S = G.NonTerminal('S', True)\n",
    "A,B = G.NonTerminals('A B')\n",
    "a,b = G.Terminals('a b')\n",
    "\n",
    "S %= A + B\n",
    "A %= a + A | a\n",
    "B %= b + B | b\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "firsts = compute_firsts(G)\n",
    "pprint(firsts)\n",
    "\n",
    "# print(inspect(firsts))\n",
    "assert firsts == {\n",
    "   a: ContainerSet(a , contains_epsilon=False),\n",
    "   b: ContainerSet(b , contains_epsilon=False),\n",
    "   S: ContainerSet(a , contains_epsilon=False),\n",
    "   A: ContainerSet(a , contains_epsilon=False),\n",
    "   B: ContainerSet(b , contains_epsilon=False),\n",
    "   Sentence(A, B): ContainerSet(a , contains_epsilon=False),\n",
    "   Sentence(a, A): ContainerSet(a , contains_epsilon=False),\n",
    "   Sentence(a): ContainerSet(a , contains_epsilon=False),\n",
    "   Sentence(b, B): ContainerSet(b , contains_epsilon=False),\n",
    "   Sentence(b): ContainerSet(b , contains_epsilon=False) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follows = compute_follows(G, firsts)\n",
    "pprint(follows)\n",
    "\n",
    "# print(inspect(follows))\n",
    "assert follows == {\n",
    "   S: ContainerSet(G.EOF , contains_epsilon=False),\n",
    "   A: ContainerSet(b , contains_epsilon=False),\n",
    "   B: ContainerSet(G.EOF , contains_epsilon=False) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = build_parsing_table(G, firsts, follows)\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramatica 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Grammar()\n",
    "S = G.NonTerminal('S', True)\n",
    "A,B,C = G.NonTerminals('A B C')\n",
    "a,b,c,d,f = G.Terminals('a b c d f')\n",
    "\n",
    "S %= a + A | B + C | f + B + f\n",
    "A %= a + A | G.Epsilon\n",
    "B %= b + B | G.Epsilon\n",
    "C %= c + C | d\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsts = compute_firsts(G)\n",
    "pprint(firsts)\n",
    "\n",
    "# print(inspect(firsts))\n",
    "assert firsts == {\n",
    "   a: ContainerSet(a , contains_epsilon=False),\n",
    "   b: ContainerSet(b , contains_epsilon=False),\n",
    "   c: ContainerSet(c , contains_epsilon=False),\n",
    "   d: ContainerSet(d , contains_epsilon=False),\n",
    "   f: ContainerSet(f , contains_epsilon=False),\n",
    "   S: ContainerSet(d, a, f, c, b , contains_epsilon=False),\n",
    "   A: ContainerSet(a , contains_epsilon=True),\n",
    "   B: ContainerSet(b , contains_epsilon=True),\n",
    "   C: ContainerSet(c, d , contains_epsilon=False),\n",
    "   Sentence(a, A): ContainerSet(a , contains_epsilon=False),\n",
    "   Sentence(B, C): ContainerSet(d, c, b , contains_epsilon=False),\n",
    "   Sentence(f, B, f): ContainerSet(f , contains_epsilon=False),\n",
    "   G.Epsilon: ContainerSet( contains_epsilon=True),\n",
    "   Sentence(b, B): ContainerSet(b , contains_epsilon=False),\n",
    "   Sentence(c, C): ContainerSet(c , contains_epsilon=False),\n",
    "   Sentence(d): ContainerSet(d , contains_epsilon=False) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follows = compute_follows(G, firsts)\n",
    "pprint(follows)\n",
    "\n",
    "# print(inspect(follows))\n",
    "assert follows == {\n",
    "   S: ContainerSet(G.EOF , contains_epsilon=False),\n",
    "   A: ContainerSet(G.EOF , contains_epsilon=False),\n",
    "   B: ContainerSet(d, f, c , contains_epsilon=False),\n",
    "   C: ContainerSet(G.EOF , contains_epsilon=False) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = build_parsing_table(G, firsts, follows)\n",
    "pprint(M)\n",
    "\n",
    "# print(inspect(M))\n",
    "assert M == {\n",
    "   ( S, a, ): [ Production(S, Sentence(a, A)), ],\n",
    "   ( S, c, ): [ Production(S, Sentence(B, C)), ],\n",
    "   ( S, b, ): [ Production(S, Sentence(B, C)), ],\n",
    "   ( S, d, ): [ Production(S, Sentence(B, C)), ],\n",
    "   ( S, f, ): [ Production(S, Sentence(f, B, f)), ],\n",
    "   ( A, a, ): [ Production(A, Sentence(a, A)), ],\n",
    "   ( A, G.EOF, ): [ Production(A, G.Epsilon), ],\n",
    "   ( B, b, ): [ Production(B, Sentence(b, B)), ],\n",
    "   ( B, c, ): [ Production(B, G.Epsilon), ],\n",
    "   ( B, f, ): [ Production(B, G.Epsilon), ],\n",
    "   ( B, d, ): [ Production(B, G.Epsilon), ],\n",
    "   ( C, c, ): [ Production(C, Sentence(c, C)), ],\n",
    "   ( C, d, ): [ Production(C, Sentence(d)), ] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = metodo_predictivo_no_recursivo(G, M)\n",
    "\n",
    "left_parse = parser([b, b, d, G.EOF])\n",
    "pprint(left_parse)\n",
    "\n",
    "# print(inspect(left_parse))\n",
    "assert left_parse == [ \n",
    "   Production(S, Sentence(B, C)),\n",
    "   Production(B, Sentence(b, B)),\n",
    "   Production(B, Sentence(b, B)),\n",
    "   Production(B, G.Epsilon),\n",
    "   Production(C, Sentence(d)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstrucción del árbol de derivación y evaluación\n",
    "- Implemente un algoritmo básico de tokenización para a partir de un `string` obtener la sequencia de tokens correspondiente. Note que los símbolos terminales de la gramática coincide con los \"tipos\" de tokens, pero no contienen el lexemas.\n",
    "- Reconstruya el árbol de derivación a partir del parse izquierdo que devuelve el parser.\n",
    "- Utilice el árbol de derivación para evaluar la expresión. Note que la estructura de la gramática causa que los operadores (+, -, \\*, /) asocien hacia la derecha, lo cual conlleva problemas si se evalúa recursivamente sin considerar tal característica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
