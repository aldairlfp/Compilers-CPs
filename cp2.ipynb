{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica #2 (Compilación)\n",
    "\n",
    "En esta clase estaremos implementando un _parser_ para el subconjunto del lenguaje `HULK` descrito en la clase anterior. Esta vez nos apoyaremos en una descripción formal del lenguaje: una gramática libre del contexto.\n",
    "\n",
    "Recordemos que una gramática `G` es un cuádruplo `<T,N,S,P>` donde:\n",
    "- `T` es el conjunto de los _terminales_ (informalmente los símbolos que realmente se imprimiran en la cadena).\n",
    "- `N` es el conjunto de los _no terminales_ (símbolos intermedios usados al generar una cadena y que deberán ser reemplazados para obtener la cadena final).\n",
    "- `S` es _el símbolo distinguido_ de la gramática (por definición, toda cadena perteneciente al lenguaje generado por la gramática deriva en 0 o más pasos del símbolo distinguido, o sea, `L(G) = { w | S =>* w }` ).\n",
    "- `P` es el conjunto de las _producciones_ de la gramática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing recursivo descendente\n",
    "\n",
    "En conferencia se discutió la idea de construir un parser partiendo de la especificación de la gramática y usando una exploración con _backtrack_. Vimos que incluso algunas gramáticas podrían ser parseadas con este mecanismo sin hacer backtrack si quiera: el parser podría seleccionar de forma determinista qué producción aplicar para obtener la derivación de la cadena. A estas gramáticas les llamamos _gramáticas LL(1)_.\n",
    "\n",
    "A continuación se presenta una implementación base del mecanismo de parsing recursivo descendente. Este facilitará la construcción _\"ad hoc\"_ de parsers para gramáticas específicas. En clases posteriores estaremos automatizando la generación del parser a partir de la descripción de la gramática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsingError(Exception):\n",
    "    \"\"\"\n",
    "    Base class for all parsing exceptions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class Token:\n",
    "    \"\"\"\n",
    "    Basic token class. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lex : str\n",
    "        Token's lexeme.\n",
    "    token_type : Enum\n",
    "        Token's type.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lex, token_type):\n",
    "        self.lex = lex\n",
    "        self.token_type = token_type\n",
    "        \n",
    "\n",
    "class Lexer:\n",
    "    \"\"\"\n",
    "    Base lexer class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        String to tokenize.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.index = 0\n",
    "        self.text = text\n",
    "        self.tokens = self.tokenize_text()\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        \"\"\"\n",
    "        Tokenize `self.text` and set it to `self.tokens`.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def next_token(self):\n",
    "        \"\"\"\n",
    "        Returns the next tokens readed by the lexer. `None` if `self.tokens` is exhausted.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            token = self.tokens[self.index]\n",
    "            self.index += 1\n",
    "            return token\n",
    "        except IndexError:\n",
    "            return None\n",
    "    \n",
    "    def is_done(self):\n",
    "        \"\"\"\n",
    "        Returns whether or not `self.tokens` is exhausted.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.tokens[self.index]\n",
    "            return False\n",
    "        except IndexError:\n",
    "            return True\n",
    "            \n",
    "\n",
    "class Parser:\n",
    "    \"\"\"\n",
    "    Base parser class.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lexer = None\n",
    "        self.left_parse = None\n",
    "        self.lookahead = None\n",
    "        \n",
    "    def parse(self, lexer):\n",
    "        \"\"\"\n",
    "        Returns a left parse given the tokens from the lexer.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.lexer = lexer\n",
    "            self.left_parse = []\n",
    "            self.lookahead = lexer.next_token().token_type\n",
    "            self.begin()\n",
    "            return self.left_parse\n",
    "        \n",
    "        except ParsingError as error:\n",
    "            print(f'Parsing error: {error}!!!')\n",
    "            print(f'Lookahead: {self.lookahead}')\n",
    "            print(f'Unfinished parse: {self.left_parse}')\n",
    "            \n",
    "        finally:\n",
    "            self.lex = None\n",
    "            self.left_parse = None\n",
    "            self.lookahead = None\n",
    "            \n",
    "    def begin(self):\n",
    "        \"\"\"\n",
    "        Begin parsing from starting symbol and match EOF.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def report(self, production):\n",
    "        \"\"\"\n",
    "        Adds production to the left parse that is being build.\n",
    "        \"\"\"\n",
    "        self.left_parse.append(production)\n",
    "        \n",
    "    def error(self, msg=None):\n",
    "        \"\"\"\n",
    "        Raises a parsing error.\n",
    "        \"\"\"\n",
    "        raise ParsingError(msg)\n",
    "        \n",
    "    def match(self, token_type):\n",
    "        \"\"\"\n",
    "        Consumes one token from the lexer if lookahead matches the given token type.\n",
    "        Raises parsing error otherwise.\n",
    "        \"\"\"\n",
    "        if token_type == self.lookahead:\n",
    "            try:\n",
    "                self.lookahead = self.lexer.next_token().token_type\n",
    "            except AttributeError:\n",
    "                self.lookahead = None\n",
    "        else:\n",
    "            self.error('Unexpected token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HULK\n",
    "\n",
    "Comenzaremos la construcción del parser para `HULK` definiendo los tokens del lenguaje. Estos tokens representan a su vez los símbolos terminales de la gramática con la que trabajará el parser. Se incluye un token `EOF` usado para marcar el fin la cadena. En `fixed_tokens` se almacenan los tokens con lexemas constantes para simplificar la implementación del parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "TokenType = Enum('TokenType', 'eof num plus minus star div opar cpar id')\n",
    "\n",
    "EOF_TOKEN = Token('$', TokenType.eof)\n",
    "\n",
    "fixed_tokens = {\n",
    "    '+'  :   Token( '+'           , TokenType.plus  ),\n",
    "    '-'  :   Token( '-'           , TokenType.minus ),\n",
    "    '*'  :   Token( '*'           , TokenType.star  ),\n",
    "    '/'  :   Token( '/'           , TokenType.div   ),\n",
    "    '('  :   Token( '('           , TokenType.opar  ),\n",
    "    ')'  :   Token( ')'           , TokenType.cpar  ),\n",
    "    'pi' :   Token( 3.14159265359 , TokenType.num   ),\n",
    "    'e'  :   Token( 2.71828182846 , TokenType.num   ),\n",
    "    'phi':   Token( 1.61803398875 , TokenType.num   ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexer\n",
    "\n",
    "La implementación del lexer es muy similar al de la clase anterior. Por ahora asumiremos que los lexemas relevantes están separados por espacios, por lo que el lexer simplemente debería separar por espacios la cadena de entrada y construir los tokens correspondientes. El lexer deberá incluir un token EOF al final de la secuencia de tokens. Esto resulta conveniente durante el proceso de parsing para evitar manejar el fin de la cadena como un caso extremo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HULKLexer(Lexer):\n",
    "\n",
    "    def tokenize_text(self):\n",
    "        tokens = []\n",
    "        text = self.text\n",
    "        \n",
    "        for item in text.split():            \n",
    "            if item.isnumeric():\n",
    "                tokens.append(Token(float(item), TokenType.num))\n",
    "            else:\n",
    "                tokens.append(fixed_tokens[item])\n",
    "            \n",
    "        tokens.append(EOF_TOKEN)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser de HULK\n",
    "\n",
    "Podemos intuir la siguiente gramática de `HULK` a partir de las consideraciones realizadas en la clase anterior:\n",
    "``` \n",
    "E --> T + E | T\n",
    "T --> F * T | F\n",
    "F --> ( E ) | n\n",
    "``` \n",
    "\n",
    "Sin embargo, rápidamente podemos notar que un parser recursivo descendente deberá dar \"un salto de fe\" para decidir qué producción, entre `E --> T + E` y `E --> T`, aplicar desde el inicio. Claro está que como ambos comienzan con `T` se puede aplazar la decisión de cuál producción aplicar hasta terminar de consumir `T`. Para evitar enfrentarnos a esto realizaremos una modificación a la gramática conocida como _eliminación de prefijos comunes_. Para ello, todas las producciones con la misma cabecera que comiencen con el mismo símbolo serán modificadas, con lo cual obtenemos:\n",
    "\n",
    "```\n",
    "E --> T X\n",
    "X --> + T X | - T X | epsilon\n",
    "T --> F Y\n",
    "Y --> * F Y | / F Y | epsilon\n",
    "F --> ( E ) | n\n",
    "```\n",
    "\n",
    "Como podremos comprobar con la implementación del parser de `HULK` según la gramática anterior, dicha gramática es _LL(1)_. En todo momento sabremos qué producción aplicar con solo ver el símbolo actual de la cadena.\n",
    "\n",
    "Para construir el parser según la gramática anterior simplemente extenderemos la clase `Parser` (usando herencia) para incluir un método por cada no terminal de la gramática. En estos métodos deberemos explorar las posibles producciones a aplicar en función del símbolo actual de la cadena (`lookahead`). Según la rama que se decida seguir, invocaremos los métodos correspondientes a los no terminales que aparezcan en la parte derecha de la producción aplicada. Por cada terminal que aparezca en la parte derecha haremos una invocación al método `match` con el tipo del token correspondiente. Este procedimiento se realiza en el orden en que aparezcan los símbolos en la producción. \n",
    "\n",
    "> **OJO:** el caso de las producciones con la forma `X --> epsilon` puede ser un tanto especial de seleccionar. Intente descubrir un forma para seleccionar dichas ramas.\n",
    "\n",
    "Es importante garantizar una invocación al método `error` en caso que ninguna de las producciones (ramas) deba ser aplicada. Esto puede saberse con un análisis manual sobre la gramática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HULKParser(Parser):\n",
    "    def begin(self):\n",
    "        self.E()\n",
    "        self.match(TokenType.eof)\n",
    "        \n",
    "    def E(self):\n",
    "        \"\"\"\n",
    "        E --> TX\n",
    "        \"\"\"\n",
    "        if self.lookahead in (TokenType.num, TokenType.opar):\n",
    "            self.report('E --> TX')\n",
    "            self.T()\n",
    "            self.X()\n",
    "            \n",
    "        else:\n",
    "            self.error('Malformed expression')\n",
    "        \n",
    "    def X(self):\n",
    "        \"\"\"\n",
    "        X --> +TX | -TX | epsilon\n",
    "        \"\"\"\n",
    "        # Insert your code here ...\n",
    "        pass\n",
    "        \n",
    "    def T(self):\n",
    "        \"\"\"\n",
    "        T --> FY\n",
    "        \"\"\"\n",
    "        # Insert your code here ...\n",
    "        pass\n",
    "            \n",
    "    def Y(self):\n",
    "        \"\"\"\n",
    "        Y --> *FY | /FY | epsilon\n",
    "        \"\"\"\n",
    "        # Insert your code here ...\n",
    "        pass\n",
    "            \n",
    "    def F(self):\n",
    "        \"\"\"\n",
    "        F --> n | (E)\n",
    "        \"\"\"\n",
    "        # Insert your code here ...\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "Cerraremos el _pipeline_ conectando el lexer y el parser, siendo el primero el encargado de preprocesar la cadena de entrada. El parser devuelve un parse izquierdo, con el cual seremos capacez de reconstruir el árbol de derivación y posteriormente evaluar la expresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_parse(text):\n",
    "    lexer = HULKLexer(text)\n",
    "    parser = HULKParser()\n",
    "    return parser.parse(lexer)\n",
    "\n",
    "assert get_left_parse('5 + 8 * 9') == [  'E --> TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> +TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> *FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> epsilon'  ]\n",
    "\n",
    "assert get_left_parse('1 - 1 + 1') == [  'E --> TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> -TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> +TX',\n",
    "                                         'T --> FY',\n",
    "                                         'F --> n',\n",
    "                                         'Y --> epsilon',\n",
    "                                         'X --> epsilon'  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adicionando funciones elementales\n",
    "\n",
    "Agreguemos funciones elementales `sin`, `cos`, `tan`, `log`, `sqrt`, etc. El llamado a funciones se hará en notación prefija, comenzando por el nombre de la función y seguido, entre paréntesis, por los argumentos, que estarán separados entre sí por _comas_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "functions = {\n",
    "    'sin': lambda x: math.sin(x),\n",
    "    'cos': lambda x: math.cos(x),\n",
    "    'tan': lambda x: math.tan(x),\n",
    "    'log': lambda x,y: math.log(x, y),\n",
    "    'sqrt': lambda x: math.sqrt(x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstrucción del árbol de derivación y evaluación\n",
    "\n",
    "- Realice las modificaciones pertinentes para que las producciones reportadas por el parser nos permitan reconstruir el árbol de derivación. Note que la implementación actual trabaja con `str` y se desechan los tokens (que son los contenedores de los lexemas).\n",
    "- Utilice el parse izquierdo y/o árbol de derivación para evaluar la expresión. Note que la estructura de la gramática causa que los operadores (+, -, \\*, /) asocien hacia la derecha, lo cual conlleva problemas si se evalúa recursivamente sin considerar tal característica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
