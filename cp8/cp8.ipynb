{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica # 8 (Compilación)\n",
    "\n",
    "En esta clase estaremos implementando un **intérprete de expresiones regulares**. Utilizaremos autómatas finitos como mecanismo reconocedor del lenguaje que denota cada expresión regular. Nos apoyaremos en las operaciones entre autómatas implementadas en las clases anteriores para ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmp.tools.automata import NFA, DFA, nfa_to_dfa\n",
    "from cmp.tools.automata import automata_union, automata_concatenation, automata_closure, automata_minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluar una expresión de determinado dominio no debería parecernos un problema salido de la nada. Desde comienzos del curso, y hasta unas clases atrás, estuvimos enfrentándonos al problema de evaluar expresiones aritméticas. Ahora nos enfrentamos a un problema similar, cambiando sumas por uniones, productos por concatenaciones, etc. Esto implica que la metodología que usaremos es la misma: obtendremos una **representación semántica**, **parseando** según una **gramática** del lenguaje de expresiones, cuyos símbolos son los **tokens** que obtenemos del **lexer**.\n",
    "\n",
    "Curiosamente, llegamos este punto con el objetivo de implementar el lexer que alimente al parser durante la contrucción del compilador. Ahora nos apoyaremos en todo lo implementado del parser para construir el lexer. Claramente, el lexer (_tokenizer_) que usaremos para construir el generador de lexer será un versión básica, a _pico y pala_, pues los tokens de las expresiones regulares son muy fáciles de extraer.\n",
    "\n",
    "### Nodos del AST\n",
    "\n",
    "Pasemos a definir los nodos del AST. Usaremos como base las clases `Node`, `AtomicNode`, `UnaryNode` y `BinaryNode` para mantener la compatibilidad con el `printer` de AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class AtomicNode(Node):\n",
    "    def __init__(self, lex):\n",
    "        self.lex = lex\n",
    "\n",
    "class UnaryNode(Node):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        \n",
    "    def evaluate(self):\n",
    "        value = self.node.evaluate() \n",
    "        return self.operate(value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def operate(value):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class BinaryNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    def evaluate(self):\n",
    "        lvalue = self.left.evaluate() \n",
    "        rvalue = self.right.evaluate()\n",
    "        return self.operate(lvalue, rvalue)\n",
    "    \n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "from cmp.ast import get_printer\n",
    "printer = get_printer(AtomicNode=AtomicNode, UnaryNode=UnaryNode, BinaryNode=BinaryNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `evaluate` debe compilar la expresión regular. Dicho de otra forma, debe devolver el `NFA` que reconoce el lenguaje denotado por la expresión regular en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.NFA at 0x28de6fc7280>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPSILON = 'ε'\n",
    "\n",
    "class EpsilonNode(AtomicNode):\n",
    "    def evaluate(self):\n",
    "        return NFA(1, [0], {})\n",
    "\n",
    "EpsilonNode(EPSILON).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.NFA at 0x28de6ff6a70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SymbolNode(AtomicNode):\n",
    "    def evaluate(self):\n",
    "        s = self.lex\n",
    "        return NFA(2, [1], {(0, s): [1]})\n",
    "\n",
    "SymbolNode('a').evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.NFA at 0x28de701e500>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ClosureNode(UnaryNode):\n",
    "    @staticmethod\n",
    "    def operate(value):\n",
    "        return automata_closure(value)\n",
    "    \n",
    "ClosureNode(SymbolNode('a')).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.NFA at 0x28de703dff0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UnionNode(BinaryNode):\n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        return automata_union(lvalue, rvalue)\n",
    "\n",
    "UnionNode(SymbolNode('a'), SymbolNode('b')).evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.NFA at 0x28de7063880>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConcatNode(BinaryNode):\n",
    "    @staticmethod\n",
    "    def operate(lvalue, rvalue):\n",
    "        return automata_concatenation(lvalue, rvalue)\n",
    "\n",
    "ConcatNode(SymbolNode('a'), SymbolNode('b')).evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gramática\n",
    "\n",
    "Habiendo definido los nodos del AST, pasemos a diseñar la gramática atributada para construirlo. Recordemos que es necesario que la gramática no sea ambigua (para ser parseable), no tener prefijos comunes ni recursividad izquierda (para ser parseable con un parser LL(1)) y que respete la prioridad de los operadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Terminals:\n",
      "\tE, T, F, A, X, Y, Z\n",
      "Terminals:\n",
      "\t|, *, (, ), symbol, ε\n",
      "Productions:\n",
      "\t[E -> T X, X -> | T X, X -> e, T -> F Y, Y -> F Y, Y -> e, F -> A Z, Z -> * Z, Z -> e, A -> symbol, A -> ε, A -> ( E )]\n"
     ]
    }
   ],
   "source": [
    "from cmp.pycompiler import Grammar\n",
    "\n",
    "G = Grammar()\n",
    "\n",
    "E = G.NonTerminal('E', True)\n",
    "T, F, A, X, Y, Z = G.NonTerminals('T F A X Y Z')\n",
    "pipe, star, opar, cpar, symbol, epsilon = G.Terminals('| * ( ) symbol ε')\n",
    "\n",
    "############################ BEGIN PRODUCTIONS ############################\n",
    "# ======================================================================= #\n",
    "#                                                                         #\n",
    "# ========================== { E --> T X } ============================== #\n",
    "#                                                                         #\n",
    "E %= T + X, lambda h,s: s[2], None, lambda h,s: s[1]\n",
    "#                                                                         #\n",
    "# =================== { X --> '|' T X | epsilon } ======================= #\n",
    "#                                                                         #\n",
    "X %= pipe + T + X, lambda h,s: s[3], None, None, lambda h,s: UnionNode(h[0], s[2])\n",
    "X %= G.Epsilon, lambda h,s: h[0]\n",
    "#                                                                         #\n",
    "# ============================ { T --> F Y } ============================ #\n",
    "#                                                                         #\n",
    "T %= F + Y, lambda h,s: s[2], None, lambda h,s: s[1]\n",
    "#                                                                         #\n",
    "# ==================== { Y --> F Y | epsilon } ========================== #\n",
    "#                                                                         #\n",
    "Y %=  F + Y, lambda h,s: s[2], None, lambda h,s: ConcatNode(h[0], s[1])\n",
    "Y %= G.Epsilon, lambda h,s: h[0]\n",
    "#                                                                         #\n",
    "# ======================= { F --> A Z } ================================= #\n",
    "#                                                                         #\n",
    "F %= A + Z, lambda h,s: s[2], None, lambda h,s: s[1]\n",
    "#                                                                         #\n",
    "# ==================== { Z --> * Z | epsilon } ========================== #\n",
    "#                                                                         #\n",
    "Z %= star + Z, lambda h,s: s[2], None, lambda h,s: ClosureNode(h[0]) \n",
    "Z %= G.Epsilon, lambda h,s: h[0]\n",
    "#                                                                         #\n",
    "# ==================== { A --> symbol | 'Epsilon' | ( E ) } ============= #\n",
    "#                                                                         #\n",
    "# ==================== { A --> symbol | 'Epsilon' | ( E ) } ============= #\n",
    "#                                                                         #\n",
    "A %= symbol, lambda h,s: SymbolNode(s[1]), symbol\n",
    "A %= epsilon, lambda h,s: EpsilonNode(s[1]), epsilon\n",
    "A %= opar + E + cpar, lambda h,s: s[2], None, None, None\n",
    "#                                                                         #\n",
    "# ======================================================================= #\n",
    "############################# END PRODUCTIONS #############################\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "Para el lexer a _pico y pala_, procederemos como de costumbre: mantendremos una colección con los tokens de lexema fijo y cualquier otro elemento será tratado como símbolo. Los lexemas no se obtendrán de separar por espacios, sino de considerar cada caracter de la cadena original como lexema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[symbol: a,\n",
       " *: *,\n",
       " (: (,\n",
       " symbol: a,\n",
       " |: |,\n",
       " symbol: b,\n",
       " ): ),\n",
       " *: *,\n",
       " symbol: c,\n",
       " symbol: d,\n",
       " |: |,\n",
       " ε: ε,\n",
       " $: $]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cmp.utils import Token\n",
    "\n",
    "def regex_tokenizer(text, G, skip_whitespaces=True):\n",
    "    tokens = []\n",
    "    # > fixed_tokens = ???\n",
    "    fixed_tokens = \"| * ( ) ε\".split()\n",
    "\n",
    "    for char in text:\n",
    "        if skip_whitespaces and char.isspace():\n",
    "            continue\n",
    "        elif char in fixed_tokens:\n",
    "            tokens.append(Token(char,G.symbDict[char]))\n",
    "        else:\n",
    "            tokens.append(Token(char,G.symbDict[\"symbol\"]))\n",
    "        \n",
    "    tokens.append(Token('$', G.EOF))\n",
    "    return tokens\n",
    "\n",
    "tokens = regex_tokenizer('a*(a|b)*cd | ε',G)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser\n",
    "\n",
    "Usaremos un parser LL(1) para obtener un parse izquierdo de la cadena (expresión regular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[E -> T X,\n",
       " T -> F Y,\n",
       " F -> A Z,\n",
       " A -> symbol,\n",
       " Z -> * Z,\n",
       " Z -> e,\n",
       " Y -> F Y,\n",
       " F -> A Z,\n",
       " A -> ( E ),\n",
       " E -> T X,\n",
       " T -> F Y,\n",
       " F -> A Z,\n",
       " A -> symbol,\n",
       " Z -> e,\n",
       " Y -> e,\n",
       " X -> | T X,\n",
       " T -> F Y,\n",
       " F -> A Z,\n",
       " A -> symbol,\n",
       " Z -> e,\n",
       " Y -> e,\n",
       " X -> e,\n",
       " Z -> * Z,\n",
       " Z -> e,\n",
       " Y -> F Y,\n",
       " F -> A Z,\n",
       " A -> symbol,\n",
       " Z -> e,\n",
       " Y -> F Y,\n",
       " F -> A Z,\n",
       " A -> symbol,\n",
       " Z -> e,\n",
       " Y -> e,\n",
       " X -> | T X,\n",
       " T -> F Y,\n",
       " F -> A Z,\n",
       " A -> ε,\n",
       " Z -> e,\n",
       " Y -> e,\n",
       " X -> e]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cmp.tools.parsing import metodo_predictivo_no_recursivo\n",
    "\n",
    "parser = metodo_predictivo_no_recursivo(G)\n",
    "left_parse = parser(tokens)\n",
    "left_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AST\n",
    "\n",
    "Para obtener el AST evaluaremos los atributos de la gramática. Esto devolverá el AST sintetizado en la producción raíz de la gramática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\__<expr> UnionNode <expr>\n",
      "\t\\__<expr> ConcatNode <expr>\n",
      "\t\t\\__<expr> ConcatNode <expr>\n",
      "\t\t\t\\__<expr> ConcatNode <expr>\n",
      "\t\t\t\t\\__<expr> ClosureNode\n",
      "\t\t\t\t\t\\__ SymbolNode: a\n",
      "\t\t\t\t\\__<expr> ClosureNode\n",
      "\t\t\t\t\t\\__<expr> UnionNode <expr>\n",
      "\t\t\t\t\t\t\\__ SymbolNode: a\n",
      "\t\t\t\t\t\t\\__ SymbolNode: b\n",
      "\t\t\t\\__ SymbolNode: c\n",
      "\t\t\\__ SymbolNode: d\n",
      "\t\\__ EpsilonNode: ε\n"
     ]
    }
   ],
   "source": [
    "from cmp.tools.evaluation import evaluate_parse\n",
    "\n",
    "ast = evaluate_parse(left_parse, tokens)\n",
    "print(printer(ast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autómata\n",
    "\n",
    "Y para obtener el autómata simplemente invocamos el método `evaluate` de la raíz del AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.NFA at 0x28de7077280>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfa = ast.evaluate()\n",
    "nfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convirtámoslo ahora en DFA para comprobar que reconoce las cadenas del lenguaje denotado por la expresión regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = nfa_to_dfa(nfa)\n",
    "display(dfa)\n",
    "\n",
    "assert dfa.recognize('')\n",
    "assert dfa.recognize('cd')\n",
    "assert dfa.recognize('aaaaacd')\n",
    "assert dfa.recognize('bbbbbcd')\n",
    "assert dfa.recognize('bbabababcd')\n",
    "assert dfa.recognize('aaabbabababcd')\n",
    "\n",
    "assert not dfa.recognize('cda')\n",
    "assert not dfa.recognize('aaaaa')\n",
    "assert not dfa.recognize('bbbbb')\n",
    "assert not dfa.recognize('ababba')\n",
    "assert not dfa.recognize('cdbaba')\n",
    "assert not dfa.recognize('cababad')\n",
    "assert not dfa.recognize('bababacc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, aplicaremos el algoritmo de minimización de autómatas para obtener una versión más compacta del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cmp.tools.automata.DFA at 0x28de4f711e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mini = automata_minimization(dfa)\n",
    "display(mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuestas\n",
    "\n",
    "- Implemente un intérprete para _expresiones regulares extendidas_ (operadores: `+`, `?`, `[ ]`, etc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
